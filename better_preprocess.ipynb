{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3012ba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/echoes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/echoes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import spacy\n",
    "from nltk.corpus import stopwords     # list of common English filler words \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import spacy.cli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baab376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"artist_data.csv\")\n",
    "#df = pd.read_csv(\"artist_data_copy.csv\")\n",
    "# Remove \"nan\" or missing values in Text column \n",
    "#df = df.copy().dropna()\n",
    "df = df.dropna(subset = ['text']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_markdown(text):\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)     # Regex to remove markdown links \n",
    "    text = re.sub(r'!\\[([^\\]]*)\\]\\([^\\)]+\\)', r'\\1', text)    # Regex to remove markdown images \n",
    "    text = re.sub(r'(\\*\\*|__)(.*?)\\1', r'\\2', text)           # Regex to remove bold formatting \n",
    "    text = re.sub(r'(\\*|_)(.*?)\\1', r'\\2', text)              # Regex to remove italic formatting \n",
    "    text = re.sub(r'^\\s*#+\\s*', '', text, flags=re.MULTILINE) # Regex to remove markdown headings \n",
    "    text = re.sub(r'```markdown', '', text)                   # Regex to remove ```markdown \n",
    "    return text \n",
    "\n",
    "df['no_md_text'] = df['text'].apply(remove_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16bfdddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d058846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    This function will do the following to each row of a column:\n",
    "    - lowercase all of the text \n",
    "    - remove punctuation \n",
    "    - tokenize the text into individual words \n",
    "    - lemmatize \n",
    "    - remove stopwords \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # tokenize + POS-tag + lemmatize all in one spacy function call \n",
    "    doc = nlp(text)\n",
    "\n",
    "    # lemmatize words (and remove stopwords)\n",
    "    lemmas_tokens = []\n",
    "    for word in doc:\n",
    "        if not word.is_stop and not word.is_punct:\n",
    "            lemmas_tokens.append(word.lemma_)\n",
    "\n",
    "    \n",
    "    return ' '.join(lemmas_tokens)    # return filtered words as a single string \n",
    "\n",
    "# extract meaningful text: only use paragraphs that contain at least a minimum number of words \n",
    "def remove_short_paragraphs(text, min_word_count = 10):\n",
    "    \"\"\"\n",
    "    This function will remove paragraphs that have less number of \n",
    "    words than the min_word_count. \n",
    "    \"\"\"\n",
    "    # 1. split text with 2x newline characters (i.e. = paragraph)\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "    # 2. remove paragraphs that contain fewer than min_word_count number of words \n",
    "    good_paragraphs = [] \n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) >= min_word_count:\n",
    "            good_paragraphs.append(p)\n",
    "\n",
    "    # 3. rejoin these paragraphs\n",
    "    return \"\\n\\n\".join(good_paragraphs)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "df['clean_text'] = df['no_md_text'].apply(remove_short_paragraphs)\n",
    "df['cleaner_text'] = df['clean_text'].apply(preprocess_text)\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06465241",
   "metadata": {},
   "source": [
    "## Get word count and lexical diversity of each article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835c2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from textblob import TextBlob \n",
    "\n",
    "def get_word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def get_lexical_diversity(text):\n",
    "    \"\"\"\n",
    "    Lexical diversity measures the relative number of unique words in text. \n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "\n",
    "    unique_tokens = set(tokens)   # a set() is like a list but only unique values \n",
    "    lex_diversity = len(unique_tokens) / len(tokens)\n",
    "    return lex_diversity\n",
    "\n",
    "\n",
    "# Get word count and character count \n",
    "df['word_count'] = df['cleaner_text'].apply(get_word_count)\n",
    "df['lexical_diversity'] = df['cleaner_text'].apply(get_lexical_diversity)\n",
    "\n",
    "df['char_count'] = df['cleaner_text'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ff3b9",
   "metadata": {},
   "source": [
    "### Split articles into 500-word sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "titles = [] \n",
    "\n",
    "for orig_idx, row in df.iterrows(): \n",
    "    words = row['cleaner_text'].split() \n",
    "    #words = row['clean_text'].split() \n",
    "\n",
    "    n_titles = math.ceil(len(words) / 500)\n",
    "\n",
    "    # then loop over each section / title (i.e. each 500 word section of article)\n",
    "    for title_i in range(n_titles):\n",
    "        start = title_i * 500  # 1st section; start = 0, 2nd section, start = 500, etc \n",
    "        end = start + 500 \n",
    "        chunk_words = words[start:end]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "\n",
    "        titles.append({\n",
    "            'line_number': row['line_number'],\n",
    "            'year' : row['year'],\n",
    "            'article_name': row['article_name'],\n",
    "            'media_type' : row['media_type'],\n",
    "            'specific_type': row['specific_type'],\n",
    "\n",
    "            'section_id': title_i + 1,\n",
    "            'section_text' : chunk_text,\n",
    "            'section_word_count' : len(chunk_words)\n",
    "        })\n",
    "\n",
    "# turn into dataframe \n",
    "df_sections = pd.DataFrame(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "828ab2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line Number</th>\n",
       "      <th>Year</th>\n",
       "      <th>Article Name</th>\n",
       "      <th>media type</th>\n",
       "      <th>section_id</th>\n",
       "      <th>section_text</th>\n",
       "      <th>section_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>I make millions from AI art — but the law has ...</td>\n",
       "      <td>article</td>\n",
       "      <td>1</td>\n",
       "      <td>million ai art law fair refik anadol support f...</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>AI’s assault on our intellectual property must...</td>\n",
       "      <td>article</td>\n",
       "      <td>1</td>\n",
       "      <td>use sharing tool find share button article cop...</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025</td>\n",
       "      <td>Photographer slams AI bots that are copying hi...</td>\n",
       "      <td>article</td>\n",
       "      <td>1</td>\n",
       "      <td>publish 730 1 mar 2025 update 1647 3 mar 2025 ...</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>The problem with AI-generated art ｜ Steven Zap...</td>\n",
       "      <td>audio</td>\n",
       "      <td>1</td>\n",
       "      <td>tanya cushman reviewer reviewer imagine year f...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>The problem with AI-generated art ｜ Steven Zap...</td>\n",
       "      <td>audio</td>\n",
       "      <td>2</td>\n",
       "      <td>anybody think possible peer dismaying experien...</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>136</td>\n",
       "      <td>2020</td>\n",
       "      <td>THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...</td>\n",
       "      <td>paper</td>\n",
       "      <td>3</td>\n",
       "      <td>digital daily life shaw kite explore contempor...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>136</td>\n",
       "      <td>2020</td>\n",
       "      <td>THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...</td>\n",
       "      <td>paper</td>\n",
       "      <td>4</td>\n",
       "      <td>states license 8 thirteen question mention ear...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>136</td>\n",
       "      <td>2020</td>\n",
       "      <td>THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...</td>\n",
       "      <td>paper</td>\n",
       "      <td>5</td>\n",
       "      <td>voice feeling idea inclusive collaborative dis...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>136</td>\n",
       "      <td>2020</td>\n",
       "      <td>THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...</td>\n",
       "      <td>paper</td>\n",
       "      <td>6</td>\n",
       "      <td>performance online time gamer potential custom...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>136</td>\n",
       "      <td>2020</td>\n",
       "      <td>THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...</td>\n",
       "      <td>paper</td>\n",
       "      <td>7</td>\n",
       "      <td>tool software serve diversify need availabilit...</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Line Number  Year                                       Article Name  \\\n",
       "0              1  2025  I make millions from AI art — but the law has ...   \n",
       "1              2  2024  AI’s assault on our intellectual property must...   \n",
       "2              3  2025  Photographer slams AI bots that are copying hi...   \n",
       "3              4  2023  The problem with AI-generated art ｜ Steven Zap...   \n",
       "4              4  2023  The problem with AI-generated art ｜ Steven Zap...   \n",
       "..           ...   ...                                                ...   \n",
       "433          136  2020  THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...   \n",
       "434          136  2020  THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...   \n",
       "435          136  2020  THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...   \n",
       "436          136  2020  THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...   \n",
       "437          136  2020  THE VOICE OF THE ARTIST IN THE AGE OF THE ALGO...   \n",
       "\n",
       "    media type  section_id                                       section_text  \\\n",
       "0      article           1  million ai art law fair refik anadol support f...   \n",
       "1      article           1  use sharing tool find share button article cop...   \n",
       "2      article           1  publish 730 1 mar 2025 update 1647 3 mar 2025 ...   \n",
       "3        audio           1  tanya cushman reviewer reviewer imagine year f...   \n",
       "4        audio           2  anybody think possible peer dismaying experien...   \n",
       "..         ...         ...                                                ...   \n",
       "433      paper           3  digital daily life shaw kite explore contempor...   \n",
       "434      paper           4  states license 8 thirteen question mention ear...   \n",
       "435      paper           5  voice feeling idea inclusive collaborative dis...   \n",
       "436      paper           6  performance online time gamer potential custom...   \n",
       "437      paper           7  tool software serve diversify need availabilit...   \n",
       "\n",
       "     section_word_count  \n",
       "0                   305  \n",
       "1                   487  \n",
       "2                   435  \n",
       "3                   500  \n",
       "4                   446  \n",
       "..                  ...  \n",
       "433                 500  \n",
       "434                 500  \n",
       "435                 500  \n",
       "436                 500  \n",
       "437                 373  \n",
       "\n",
       "[438 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "415feb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sections.to_csv(\"Sections_of_Articles.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4437623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
